{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import h5py\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_root, target_size=(512, 512), bgr2rgb=True):\n",
    "        self.hypers = []\n",
    "        self.bgrs = []\n",
    "        hyper_data_path = f'{data_root}/Train_spectral/'\n",
    "        bgr_data_path = f'{data_root}/Train_RGB/'\n",
    "        with open(f'{data_root}/split_txt/Train_list.txt', 'r') as fin:\n",
    "            hyper_list = [line.replace('\\n', '.mat') for line in fin]\n",
    "            bgr_list = [line.replace('mat','jpg') for line in hyper_list]\n",
    "        hyper_list.sort()\n",
    "        bgr_list.sort()\n",
    "        print(f'len(hyper_Train) of ntire2022 dataset:{len(hyper_list)}')\n",
    "        print(f'len(bgr_Train) of ntire2022 dataset:{len(bgr_list)}')\n",
    "        for i in range(len(hyper_list)):\n",
    "            hyper_path = hyper_data_path + hyper_list[i]\n",
    "            if 'mat' not in hyper_path:\n",
    "                continue\n",
    "            with h5py.File(hyper_path, 'r') as mat:\n",
    "                hyper = np.float32(np.array(mat['cube']))\n",
    "            hyper = np.transpose(hyper, [0, 2, 1])\n",
    "            bgr_path = bgr_data_path + bgr_list[i]\n",
    "            assert hyper_list[i].split('.')[0] == bgr_list[i].split('.')[0], 'Hyper and RGB come from different scenes.'\n",
    "            bgr = cv2.imread(bgr_path)\n",
    "            if bgr2rgb:\n",
    "                bgr = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "            bgr = np.float32(bgr)\n",
    "            bgr = (bgr - bgr.min()) / (bgr.max() - bgr.min())\n",
    "            bgr = np.transpose(bgr, [2, 0, 1])\n",
    "            self.hypers.append(hyper)\n",
    "            self.bgrs.append(bgr)\n",
    "            mat.close()\n",
    "            print(f'Ntire2022 scene {i} is loaded.')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hyper = self.hypers[idx]\n",
    "        bgr = self.bgrs[idx]\n",
    "        target_size = (512,512)\n",
    "        # Resize both hyper and bgr images to the target size\n",
    "        hyper = cv2.resize(hyper.transpose(1, 2, 0), target_size).transpose(2, 0, 1)\n",
    "        bgr = cv2.resize(bgr.transpose(1, 2, 0), target_size).transpose(2, 0, 1)\n",
    "        \n",
    "        return np.ascontiguousarray(bgr), np.ascontiguousarray(hyper)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypers)\n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, data_root, target_size=(512, 512), bgr2rgb=True):\n",
    "        self.hypers = []\n",
    "        self.bgrs = []\n",
    "        hyper_data_path = f'{data_root}/Valid_spectral/'\n",
    "        bgr_data_path = f'{data_root}/Valid_RGB/'\n",
    "        with open(f'{data_root}/split_txt/valid_list.txt', 'r') as fin:\n",
    "            hyper_list = [line.replace('\\n', '.mat') for line in fin]\n",
    "            bgr_list = [line.replace('mat','jpg') for line in hyper_list]\n",
    "        hyper_list.sort()\n",
    "        bgr_list.sort()\n",
    "        print(f'len(hyper_valid) of ntire2022 dataset:{len(hyper_list)}')\n",
    "        print(f'len(bgr_valid) of ntire2022 dataset:{len(bgr_list)}')\n",
    "        for i in range(len(hyper_list)):\n",
    "            hyper_path = hyper_data_path + hyper_list[i]\n",
    "            if 'mat' not in hyper_path:\n",
    "                continue\n",
    "            with h5py.File(hyper_path, 'r') as mat:\n",
    "                hyper = np.float32(np.array(mat['cube']))\n",
    "            hyper = np.transpose(hyper, [0, 2, 1])\n",
    "            bgr_path = bgr_data_path + bgr_list[i]\n",
    "            assert hyper_list[i].split('.')[0] == bgr_list[i].split('.')[0], 'Hyper and RGB come from different scenes.'\n",
    "            bgr = cv2.imread(bgr_path)\n",
    "            if bgr2rgb:\n",
    "                bgr = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "            bgr = np.float32(bgr)\n",
    "            bgr = (bgr - bgr.min()) / (bgr.max() - bgr.min())\n",
    "            bgr = np.transpose(bgr, [2, 0, 1])\n",
    "            self.hypers.append(hyper)\n",
    "            self.bgrs.append(bgr)\n",
    "            mat.close()\n",
    "            print(f'Ntire2022 scene {i} is loaded.')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hyper = self.hypers[idx]\n",
    "        bgr = self.bgrs[idx]\n",
    "#         print(hyper.shape)\n",
    "        \n",
    "        target_size = (512,512)\n",
    "        # Resize both hyper and bgr images to the target size\n",
    "        hyper = cv2.resize(hyper.transpose(1, 2, 0), target_size).transpose(2, 0, 1)\n",
    "        bgr = cv2.resize(bgr.transpose(1, 2, 0), target_size).transpose(2, 0, 1)\n",
    "#         print(hyper.shape)\n",
    "        return np.ascontiguousarray(bgr), np.ascontiguousarray(hyper)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/user/Desktop/Research/Datasets/Manual_split_NTIRE_2022/split_txt/Train_list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m data_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/user/Desktop/Research/Datasets/Manual_split_NTIRE_2022\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create instances of TrainDataset and ValidDataset\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTrainDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbgr2rgb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m ValidDataset(data_root\u001b[38;5;241m=\u001b[39mdata_root, bgr2rgb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create DataLoader instances for both datasets\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mTrainDataset.__init__\u001b[0;34m(self, data_root, target_size, bgr2rgb)\u001b[0m\n\u001b[1;32m     11\u001b[0m hyper_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Train_spectral/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m bgr_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Train_RGB/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_root\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/split_txt/Train_list.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m     14\u001b[0m     hyper_list \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fin]\n\u001b[1;32m     15\u001b[0m     bgr_list \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmat\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m hyper_list]\n",
      "File \u001b[0;32m~/anaconda3/envs/research/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/user/Desktop/Research/Datasets/Manual_split_NTIRE_2022/split_txt/Train_list.txt'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the dataset path\n",
    "data_root = \"C:/Users/user/Desktop/Research/Datasets/Manual_split_NTIRE_2022\"\n",
    "\n",
    "# Create instances of TrainDataset and ValidDataset\n",
    "train_dataset = TrainDataset(data_root=data_root, bgr2rgb=True)\n",
    "valid_dataset = ValidDataset(data_root=data_root, bgr2rgb=True)\n",
    "\n",
    "# Create DataLoader instances for both datasets\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load one batch of data from each DataLoader\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "valid_images, valid_labels = next(iter(valid_loader))\n",
    "\n",
    "# Print the shapes of the loaded data\n",
    "print(\"Train Images Shape:\", train_images.shape)\n",
    "print(\"Train Labels Shape:\", train_labels.shape)\n",
    "print(\"Valid Images Shape:\", valid_images.shape)\n",
    "print(\"Valid Labels Shape:\", valid_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(valid_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = train_dataset[0]\n",
    "x, y = sample[0], sample[1]\n",
    "\n",
    "# Create subplots for the 3-channel input (RGB)\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(x.shape[0]):  # Loop through channels\n",
    "    plt.subplot(1, 3, i + 1)  # Adjust for the number of input channels (3)\n",
    "    channel_data = x[i]\n",
    "    plt.imshow(channel_data, cmap='gray')  # Display each channel in grayscale\n",
    "    plt.title(f'Input Channel {i + 1}')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Create subplots for the 31-channel output (Hyperspectral)\n",
    "plt.figure(figsize=(18, 18))  # Adjust size as needed\n",
    "for i in range(y.shape[0]):  # Loop through channels\n",
    "    plt.subplot(6, 6, i + 1)  # Adjust subplot grid for 31 channels\n",
    "    channel_data = y[i]\n",
    "    plt.imshow(channel_data, cmap='gray')  # Display each channel in grayscale\n",
    "    plt.title(f'Output Channel {i + 1}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom SAM LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SAMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAMLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted_hsi, ground_truth_hsi):\n",
    "        # Ensure the input tensors have the same shape\n",
    "        assert predicted_hsi.shape == ground_truth_hsi.shape, \"Input tensor shapes must match.\"\n",
    "\n",
    "        # Reshape tensors if necessary\n",
    "        if predicted_hsi.dim() == 4:  # If the tensors are 4D, flatten them\n",
    "            predicted_hsi = predicted_hsi.view(predicted_hsi.size(0), -1)\n",
    "            ground_truth_hsi = ground_truth_hsi.view(ground_truth_hsi.size(0), -1)\n",
    "\n",
    "        # Calculate the dot product and magnitudes\n",
    "        dot_product = torch.sum(predicted_hsi * ground_truth_hsi, dim=1)\n",
    "        magnitude_pred = torch.norm(predicted_hsi, dim=1)\n",
    "        magnitude_gt = torch.norm(ground_truth_hsi, dim=1)\n",
    "\n",
    "        # Calculate the cosine of the spectral angle\n",
    "        cosine_theta = dot_product / (magnitude_pred * magnitude_gt)\n",
    "\n",
    "        # Calculate the spectral angle in radians\n",
    "        spectral_angle = torch.acos(cosine_theta)\n",
    "\n",
    "        # Return the mean spectral angle as the loss\n",
    "        return torch.mean(spectral_angle)\n",
    "    \n",
    "class Loss_MRAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss_MRAE, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, label):\n",
    "        assert outputs.shape == label.shape\n",
    "        error = torch.abs(outputs - label) / label\n",
    "        mrae = torch.mean(error.view(-1))\n",
    "        return mrae\n",
    "\n",
    "class Loss_RMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss_RMSE, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, label):\n",
    "        assert outputs.shape == label.shape\n",
    "        error = outputs-label\n",
    "        sqrt_error = torch.pow(error,2)\n",
    "        rmse = torch.sqrt(torch.mean(sqrt_error.view(-1)))\n",
    "        return rmse\n",
    "\n",
    "class Loss_PSNR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss_PSNR, self).__init__()\n",
    "\n",
    "    def forward(self, im_true, im_fake, data_range=255):\n",
    "        N = im_true.size()[0]\n",
    "        C = im_true.size()[1]\n",
    "        H = im_true.size()[2]\n",
    "        W = im_true.size()[3]\n",
    "        Itrue = im_true.clamp(0., 1.).mul_(data_range).resize_(N, C * H * W)\n",
    "        Ifake = im_fake.clamp(0., 1.).mul_(data_range).resize_(N, C * H * W)\n",
    "        mse = nn.MSELoss(reduce=False)\n",
    "        err = mse(Itrue, Ifake).sum(dim=1, keepdim=True).div_(C * H * W)\n",
    "        psnr = 10. * torch.log((data_range ** 2) / err) / np.log(10.)\n",
    "        return torch.mean(psnr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combined loss L1loss and SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.sam_loss = SAMLoss()\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        loss_l1 = self.l1_loss(outputs, labels)\n",
    "        loss_sam = self.sam_loss(outputs, labels)\n",
    "        combined_loss = self.alpha * loss_l1 + self.beta * loss_sam\n",
    "        return combined_loss\n",
    "    \n",
    "class SRLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        super(SRLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight for SAM loss\n",
    "        self.beta = beta    # Weight for RMSE loss\n",
    "\n",
    "        # Define the SAM and RMSE loss functions\n",
    "        self.sam_loss = SAMLoss()\n",
    "        self.rmse_loss = Loss_RMSE()\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        # Calculate the SAM loss\n",
    "        sam_loss = self.sam_loss(outputs, labels)\n",
    "\n",
    "        # Calculate the RMSE loss\n",
    "        rmse_loss = self.rmse_loss(outputs, labels)\n",
    "\n",
    "        # Combine the SAM and RMSE losses using weights (alpha and beta)\n",
    "        combined_loss = self.alpha * sam_loss + self.beta * rmse_loss\n",
    "\n",
    "        return combined_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperAttention model with spacial, spectral attention and fft block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SpectralAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpectralAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool = F.avg_pool2d(x, (x.size(2), x.size(3)))\n",
    "        channel_weights = self.sigmoid(self.conv1(avg_pool))\n",
    "        x_att = x * channel_weights\n",
    "        return x_att\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.batch_norm = nn.BatchNorm2d(in_channels)  # Batch Normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)  # Apply Batch Normalization\n",
    "        spatial_weights = self.sigmoid(self.conv1(x))\n",
    "        x_att = x * spatial_weights\n",
    "        return x_att\n",
    "    \n",
    "class OutputNormalization(nn.Module):\n",
    "    def forward(self, x):\n",
    "        min_val = torch.min(x)\n",
    "        range_val = torch.max(x) - min_val\n",
    "        return (x - min_val) / (range_val + 1e-8)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.spacial_enc1 = SpatialAttention(64)\n",
    "        self.spectral_enc1 = SpectralAttention(64)\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.spacial_enc2 = SpatialAttention(128)\n",
    "        self.spectral_enc2 = SpectralAttention(128)\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.spacial_enc3 = SpatialAttention(256)\n",
    "        self.spectral_enc3 = SpectralAttention(256)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.spacial_bot = SpatialAttention(256)\n",
    "        self.spectral_bot = SpectralAttention(256)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)  # Upsampling\n",
    "        )\n",
    "        \n",
    "        self.spacial_dec1 = SpatialAttention(128)\n",
    "        self.spectral_dec1 = SpectralAttention(128)\n",
    "        \n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SpectralAttention(128),  # Spectral attention mechanism\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)  # Upsampling\n",
    "        )\n",
    "        \n",
    "        self.spacial_dec2 = SpatialAttention(64)\n",
    "        self.spectral_dec2 = SpectralAttention(64)\n",
    "        \n",
    "        self.output_norm = OutputNormalization()\n",
    "        \n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "#         print(enc1.shape)\n",
    "        enc2 = self.enc2(enc1)\n",
    "#         print(enc2.shape)\n",
    "        enc3 = self.enc3(enc2)\n",
    "#         print(enc3.shape)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(enc3)\n",
    "#         print(bottleneck.shape)\n",
    "\n",
    "        # Decoder\n",
    "        dec1 = self.dec1(torch.cat([enc3, bottleneck], dim=1))\n",
    "#         print(dec1.shape)\n",
    "        dec2 = self.dec2(torch.cat([enc2, dec1], dim=1))\n",
    "#         print(dec2.shape)\n",
    "        enc1 = self.spacial_enc1(enc1)\n",
    "    \n",
    "        dec3 = self.dec3(torch.cat([enc1, dec2], dim=1))\n",
    "#         print(dec3.shape)\n",
    "        dec3 = self.output_norm(dec3)\n",
    "    \n",
    "        return dec3\n",
    "    \n",
    "    def adjust_channels(self, x, target_channels):\n",
    "        if x.size(1) == target_channels:\n",
    "            return x\n",
    "        else:\n",
    "            # Move the convolutional layer to the same device as the input tensor\n",
    "            conv_layer = nn.Conv2d(x.size(1), target_channels, kernel_size=1).to(x.device)\n",
    "            return conv_layer(x)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the U-Net model with skip connections, loss function, and optimizer\n",
    "model= UNet(in_channels=3, out_channels=31).to(device).float()\n",
    "model = model.float()\n",
    "# criterion = nn.L1Loss()  # You can use a suitable loss function for your task\n",
    "criterion =SAMLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Model, criterion, and other components initialization code remains the same\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "verbose_logs = []\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 1\n",
    "learning_rate = 0.001  # Define the learning rate before the optimizer\n",
    "num_epochs = 2000  # Number of epochs\n",
    "checkpoint_interval = 200\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HyperAttention(in_channels=3, out_channels=31).to(device).float()\n",
    "\n",
    "# Initialize optimizer with beta parameters\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "\n",
    "# Define and initialize the cosine annealing scheduler\n",
    "T_max = 60  # The number of epochs for cosine annealing\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max)\n",
    "# criterion = SAMLoss()\n",
    "criterion = SRLoss()\n",
    "\n",
    "# Training and validation functions remain the same\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_epoch = 0\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Function to validate the model\n",
    "from piq import ssim, psnr\n",
    "\n",
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_ssim = 0.0\n",
    "    running_psnr = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(valid_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Normalize outputs and labels to [0, 1] for SSIM and PSNR calculations\n",
    "            outputs = torch.clamp(outputs, 0, 1)\n",
    "            labels = torch.clamp(labels, 0, 1)\n",
    "\n",
    "            # Calculate SSIM and PSNR for each batch\n",
    "            batch_ssim = ssim(outputs, labels, data_range=1.0).item()\n",
    "            batch_psnr = psnr(outputs, labels, data_range=1.0).item()\n",
    "            running_ssim += batch_ssim\n",
    "            running_psnr += batch_psnr\n",
    "\n",
    "        avg_loss = running_loss / len(valid_loader)\n",
    "        avg_ssim = running_ssim / len(valid_loader)\n",
    "        avg_psnr = running_psnr / len(valid_loader)\n",
    "        \n",
    "    return avg_loss, avg_ssim, avg_psnr\n",
    "\n",
    "# Main training loop with plotting\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    valid_loss, valid_ssim, valid_psnr = validate_model(model, valid_loader, criterion, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "\n",
    "    # Displaying the metrics\n",
    "    epoch_log = f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Valid SSIM: {valid_ssim:.4f}, Valid PSNR: {valid_psnr:.4f}'\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Check and save the best model\n",
    "    if valid_loss < best_val_loss:\n",
    "        best_val_loss = valid_loss\n",
    "        best_model_path = f'../Models/best_FocusNet_norm.pth'\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        epoch_log += f\", Best model saved with validation loss: {best_val_loss:.4f} at {best_model_path}\"\n",
    "        print(f\"Best model saved with validation loss: {best_val_loss:.4f}.......\")\n",
    "        best_model_epoch = epoch  # Update the best model epoch\n",
    "\n",
    "\n",
    "    # Save checkpoint at specific intervals\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_filename = f'../Models/checkpoints/FocusNet_norm_epoch_{epoch+1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_filename)\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "        \n",
    "    # Update verbose logs\n",
    "    verbose_logs.append(epoch_log)\n",
    "\n",
    "    # Live plot\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "\n",
    "    # Add a marker for the best model\n",
    "    if best_model_epoch > 0:\n",
    "        plt.scatter(best_model_epoch, val_losses[best_model_epoch], color='red', label='Best Model')\n",
    "\n",
    "    plt.title(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Print all verbose logs\n",
    "    for log in verbose_logs:\n",
    "        print(log)\n",
    "\n",
    "# Save the final model\n",
    "final_model_filename = '../Models/final_FocusNet_norm.pth'\n",
    "torch.save(model.state_dict(), final_model_filename)\n",
    "print(\"Final model saved & training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), '../../Models/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of cubes and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HyperAttention(in_channels=3, out_channels=31).to(device).float()  # Adjust out_channels to match the target label size\n",
    "# model.load_state_dict(torch.load('../Models/best_HyperAttention_fft.pth', map_location=device))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on a single file with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# image_path = \"C:/Users/user/Desktop/HSI/dataset/ICASSP2024-SPGC/Hyper-Skin(RGB,VIS)/test/RGB_CIE/p012_neutral_front.jpg\"\n",
    "image_path = 'C:/Users/user/Desktop/Research/NTIRE_2022/NTIRE_2022/Test_RGB/ARAD_1K_0951.jpg'\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations to convert and preprocess the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # Resize the image to match your model's input size\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "# Apply the transformations to the image\n",
    "input_data = preprocess(image)\n",
    "\n",
    "# Add an extra dimension to match the batch size (1 image in this case)\n",
    "input_data = input_data.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_data = input_data.to(device)\n",
    "    output = model(input_data)\n",
    "    \n",
    "print('input data max',input_data.max())\n",
    "print('input data min',input_data.min())\n",
    "print(output.shape)\n",
    "print('output max',output.max())\n",
    "print('output min',output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save .mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'output' is the result from your model and has shape [1, 31, 482, 512]\n",
    "# Remove the batch dimension if it exists\n",
    "if len(output.shape) > 3:\n",
    "    output = output.squeeze(0)\n",
    "\n",
    "num_channels = output.shape[0]\n",
    "num_rows = 8  # You can adjust this based on your preference\n",
    "num_cols = (num_channels + num_rows - 1) // num_rows  # Calculate the number of columns\n",
    "\n",
    "plt.figure(figsize=(16, 12))  # Adjust the figure size as needed\n",
    "\n",
    "for i in range(num_channels):\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    channel_data = output[i].cpu().numpy()  # Convert the channel data to a NumPy array\n",
    "    plt.imshow(channel_data, cmap='gray')  # Display the image in grayscale\n",
    "    plt.title(f'Channel {i + 1}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define the path to the .mat file where you want to save the output as a cube\n",
    "output_mat_file = \"rgb_2.mat\"  # Provide the desired file path\n",
    "\n",
    "# Assuming 'output' is a PyTorch tensor\n",
    "output_np = output.cpu().numpy()  # Convert the CUDA tensor to a NumPy array\n",
    "\n",
    "# Create the .mat file using h5py and save the data without rotation\n",
    "with h5py.File(output_mat_file, 'w') as mat_file:\n",
    "    # Save the data without rotation\n",
    "    mat_file.create_dataset('cube', data=output_np[0])  # Assuming you want to save the first element\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the output predicted .mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Define the path to the .mat file\n",
    "mat_file_path = \"rgb_2.mat\"\n",
    "\n",
    "# Open the .mat file using h5py\n",
    "mat_contents = h5py.File(mat_file_path, 'r')\n",
    "\n",
    "# Access the data by the key \"cube\"\n",
    "cube_data = mat_contents[\"cube\"]\n",
    "\n",
    "# Convert the data to a NumPy array\n",
    "output = cube_data[()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if the output has a batch dimension and remove it if it exists\n",
    "if len(output.shape) > 3:\n",
    "    output = output[0]\n",
    "\n",
    "num_channels = output.shape[0]\n",
    "num_rows = 8  # You can adjust this based on your preference\n",
    "num_cols = (num_channels - 1) // num_rows + 1  # Calculate the number of columns\n",
    "\n",
    "plt.figure(figsize=(16, 12))  # You can adjust the figure size as needed\n",
    "\n",
    "for i in range(num_channels):\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    channel_data = output[i]  # Use the NumPy array directly\n",
    "    plt.imshow(channel_data, cmap='gray')\n",
    "    plt.title(f'Channel {i + 1}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigger visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the .mat file\n",
    "mat_file_path = \"rgb_2.mat\"\n",
    "\n",
    "# Open the .mat file using h5py\n",
    "mat_contents = h5py.File(mat_file_path, 'r')\n",
    "\n",
    "# Access the data by the key \"cube\"\n",
    "cube_data = mat_contents[\"cube\"]\n",
    "\n",
    "# Convert the data to a NumPy array\n",
    "output = cube_data[()]\n",
    "\n",
    "# Check if the output has a batch dimension and remove it if it exists\n",
    "if len(output.shape) > 3:\n",
    "    output = output[0]\n",
    "\n",
    "num_channels = output.shape[0]\n",
    "\n",
    "# Set the figure size to display images in a single column\n",
    "plt.figure(figsize=(6, 6 * num_channels))  # Adjust the height based on the number of channels\n",
    "\n",
    "for i in range(num_channels):\n",
    "    plt.subplot(num_channels, 1, i + 1)  # Display images in a single column\n",
    "    channel_data = output[i]  # Convert the tensor to a NumPy array\n",
    "#     channel_data = np.rot90(channel_data, k=3)  # Rotate counterclockwise by 90 degrees\n",
    "    plt.imshow(channel_data, cmap='gray', origin='upper')  # Specify origin='upper' to prevent rotation\n",
    "    plt.title(f'Channel {i + 1}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictions on all test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# import torchvision.transforms as transforms\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# model = HyperAttention(in_channels=4, out_channels=61).to(device).float()  # Adjust out_channels to match the target label size\n",
    "# model.load_state_dict(torch.load('../Models/Aug_HyperAttention_fft_470e.pth'))\n",
    "# model.eval() \n",
    "\n",
    "# input_folder = \"../Dataset/ICASSP2024-SPGC/Hyper-Skin(MSI,NIR)/test/MSI_CIE\" # Replace this with the path the input test folder\n",
    "# output_folder = \"./output/Aug_HyperAttention_fft_470e\" # Replace with the path to your output folder\n",
    "\n",
    "# # Make sure the output folder exists\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # Define the preprocessing transformation\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize((1024, 1024)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # Loop through all the image files in the input folder\n",
    "# for filename in os.listdir(input_folder):\n",
    "#     if filename.endswith(\".mat\"):  # Assuming you have .mat files for 4-channel input\n",
    "#         # Load the .mat file\n",
    "#         mat_file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "#         # Load the .mat file using h5py\n",
    "#         mat_file = h5py.File(mat_file_path, 'r')\n",
    "\n",
    "#         # Access and load the 'cube' dataset\n",
    "#         input_data = mat_file['cube']  # Use the correct dataset name\n",
    "\n",
    "#         # Convert the data to a NumPy array\n",
    "#         input_data = np.array(input_data)\n",
    "\n",
    "#         # Convert the NumPy array to a PyTorch tensor\n",
    "#         input_data = torch.from_numpy(input_data)\n",
    "\n",
    "#         # Add an extra dimension to match the batch size (1 image in this case)\n",
    "#         input_data = input_data.unsqueeze(0)\n",
    "\n",
    "#         # Ensure that the input data and model have the same data type\n",
    "#         input_data = input_data.to(device).float()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             # Get model predictions\n",
    "#             output = model(input_data)\n",
    "\n",
    "#         # Create the output .mat file\n",
    "#         output_mat_file = os.path.join(output_folder, os.path.splitext(filename)[0] + \".mat\")\n",
    "\n",
    "#         # Convert the output to a NumPy array\n",
    "#         output_np = output.cpu().numpy()  # Move the output to the CPU and then convert to NumPy\n",
    "\n",
    "#         # Save the data as a .mat file\n",
    "#         with h5py.File(output_mat_file, 'w') as mat_file:\n",
    "#             mat_file.create_dataset('cube', data=output_np[0])\n",
    "\n",
    "#         print(f\"Processed {filename} and saved as {os.path.basename(output_mat_file)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## val sam calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from piq import ssim\n",
    "\n",
    "criterion = SAMLoss()\n",
    "# Define the RMSE Loss Function\n",
    "def rmse_loss(outputs, targets):\n",
    "    mse = torch.mean((outputs - targets) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Define the PSNR Loss Function\n",
    "def psnr_loss(outputs, targets, max_pixel=1.0):  # Assuming normalized data [0, 1]\n",
    "    mse = torch.mean((outputs - targets) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 20 * math.log10(max_pixel) - 10 * math.log10(mse)\n",
    "    return psnr\n",
    "\n",
    "# Assuming the model is already in evaluation mode and loaded\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to store total losses\n",
    "total_sam_loss = 0.0\n",
    "total_ssim = 0.0\n",
    "total_rmse = 0.0\n",
    "total_psnr = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x_val, y_val) in enumerate(valid_loader):\n",
    "        x_val, y_val = x_val.to(device).float(), y_val.to(device).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs_val = model(x_val)\n",
    "\n",
    "        # Ensure output size matches target size\n",
    "        outputs_val = torch.nn.functional.interpolate(outputs_val, size=y_val.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Min-max normalization for outputs and targets\n",
    "        outputs_val_norm = (outputs_val - outputs_val.min()) / (outputs_val.max() - outputs_val.min() + 1e-8)\n",
    "        y_val_norm = (y_val - y_val.min()) / (y_val.max() - y_val.min() + 1e-8)\n",
    "\n",
    "        # Calculate losses\n",
    "        sam_loss = criterion(outputs_val_norm, y_val_norm)\n",
    "        ssim_val = ssim(outputs_val_norm, y_val_norm, data_range=1.0)\n",
    "        rmse_val = rmse_loss(outputs_val_norm, y_val_norm)\n",
    "        psnr_val = psnr_loss(outputs_val_norm, y_val_norm)\n",
    "\n",
    "        # Accumulate the total losses\n",
    "        total_sam_loss += sam_loss.item()\n",
    "        total_ssim += ssim_val.item()\n",
    "        total_rmse += rmse_val.item()\n",
    "        total_psnr += psnr_val\n",
    "\n",
    "# Calculate the average losses over all batches\n",
    "avg_sam_loss = total_sam_loss / len(valid_loader)\n",
    "avg_ssim = total_ssim / len(valid_loader)\n",
    "avg_rmse = total_rmse / len(valid_loader)\n",
    "avg_psnr = total_psnr / len(valid_loader)\n",
    "\n",
    "# Print the average losses\n",
    "print(f\"Average SAM Loss: {avg_sam_loss:.4f}\")\n",
    "print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "print(f\"Average PSNR: {avg_psnr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model is already loaded and in evaluation mode\n",
    "model.eval()\n",
    "criterion = SAMLoss()\n",
    "\n",
    "# Define a variable to store the total loss\n",
    "total_val_loss = 0.0\n",
    "\n",
    "# Iterate over the validation loader\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x_val, y_val) in enumerate(valid_loader):\n",
    "        x_val, y_val = x_val.to(device).float(), y_val.to(device).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs_val = model(x_val)\n",
    "        \n",
    "        # You may need to apply the same interpolation to outputs_val as in the training loop\n",
    "        outputs_val = torch.nn.functional.interpolate(outputs_val, size=(482, 512), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        val_loss = criterion(outputs_val, y_val)\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "# Calculate the average validation loss\n",
    "avg_val_loss = total_val_loss / len(valid_loader)\n",
    "\n",
    "# Print or use the average validation loss as needed\n",
    "print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train sam loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model is already loaded and in evaluation mode\n",
    "model.eval()\n",
    "criterion = SAMLoss()\n",
    "# Define a variable to store the total loss\n",
    "total_train_loss = 0.0\n",
    "\n",
    "# Iterate over the training loader\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x_train, y_train) in enumerate(train_loader):  # Use train_loader instead of val_loader\n",
    "        x_train, y_train = x_train.to(device).float(), y_train.to(device).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs_train = model(x_train)\n",
    "        \n",
    "        # You may need to apply the same interpolation to outputs_train as in the training loop\n",
    "        outputs_train = torch.nn.functional.interpolate(outputs_train,size=(482, 512), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        train_loss = criterion(outputs_train, y_train)\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "# Calculate the average training loss\n",
    "avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "# Print or use the average training loss as needed\n",
    "print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the model is in evaluation mode and loaded\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to store total losses for training data\n",
    "total_sam_loss_train = 0.0\n",
    "total_ssim_train = 0.0\n",
    "total_rmse_train = 0.0\n",
    "total_psnr_train = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x_train, y_train) in enumerate(train_loader):\n",
    "        x_train, y_train = x_train.to(device).float(), y_train.to(device).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs_train = model(x_train)\n",
    "\n",
    "        # Ensure output size matches target size\n",
    "        outputs_train = torch.nn.functional.interpolate(outputs_train, size=y_train.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Min-max normalization for outputs and targets\n",
    "        outputs_train_norm = (outputs_train - outputs_train.min()) / (outputs_train.max() - outputs_train.min() + 1e-8)\n",
    "        y_train_norm = (y_train - y_train.min()) / (y_train.max() - y_train.min() + 1e-8)\n",
    "\n",
    "        # Calculate losses\n",
    "        sam_loss_train = criterion(outputs_train_norm, y_train_norm)\n",
    "        ssim_train = ssim(outputs_train_norm, y_train_norm, data_range=1.0)\n",
    "        rmse_train = rmse_loss(outputs_train_norm, y_train_norm)\n",
    "        psnr_train = psnr_loss(outputs_train_norm, y_train_norm)\n",
    "\n",
    "        # Accumulate the total losses for training data\n",
    "        total_sam_loss_train += sam_loss_train.item()\n",
    "        total_ssim_train += ssim_train.item()\n",
    "        total_rmse_train += rmse_train.item()\n",
    "        total_psnr_train += psnr_train\n",
    "\n",
    "# Calculate the average losses over all training batches\n",
    "avg_sam_loss_train = total_sam_loss_train / len(train_loader)\n",
    "avg_ssim_train = total_ssim_train / len(train_loader)\n",
    "avg_rmse_train = total_rmse_train / len(train_loader)\n",
    "avg_psnr_train = total_psnr_train / len(train_loader)\n",
    "\n",
    "# Print the average losses for training data\n",
    "print(f\"Average SAM Loss on Training Data: {avg_sam_loss_train:.4f}\")\n",
    "print(f\"Average SSIM on Training Data: {avg_ssim_train:.4f}\")\n",
    "print(f\"Average RMSE on Training Data: {avg_rmse_train:.4f}\")\n",
    "print(f\"Average PSNR on Training Data: {avg_psnr_train:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
